"""
MBTI Personality Analysis App - Streamlit Interface
"""

import streamlit as st
import json
import os
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
import plotly.express as px
import pandas as pd
from pathlib import Path
import time
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('app.log')
    ]
)
logger = logging.getLogger(__name__)

# Import local modules
from src.pipeline import MBTIPipeline
from src.embedding import SemanticEmbedder
from src.style_embedding import StyleEmbedder

# Page config
st.set_page_config(
    page_title="MBTI Personality Analyzer",
    page_icon="üß†",
    layout="wide",
    initial_sidebar_state="expanded"
)

# MBTI Questions
MBTI_QUESTIONS = [
    "Trong m·ªôt bu·ªïi ti·ªác ƒë√¥ng ng∆∞·ªùi, b·∫°n th∆∞·ªùng l√†m g√¨ ngay khi b∆∞·ªõc v√†o v√† ƒëi·ªÅu ƒë√≥ ·∫£nh h∆∞·ªüng th·∫ø n√†o ƒë·∫øn m·ª©c nƒÉng l∆∞·ª£ng c·ªßa b·∫°n?",
    "K·ªÉ v·ªÅ l·∫ßn b·∫°n c·∫ßn n·∫°p l·∫°i nƒÉng l∆∞·ª£ng sau m·ªôt ng√†y cƒÉng th·∫≥ng; b·∫°n ch·ªçn ho·∫°t ƒë·ªông g√¨ v√† t·∫°i sao?",
    "H√£y m√¥ t·∫£ c√°ch b·∫°n quan s√°t m·ªôt c·∫£nh thi√™n nhi√™n quen thu·ªôc ‚Äì b·∫°n ch√∫ √Ω chi ti·∫øt c·ª• th·ªÉ n√†o ƒë·∫ßu ti√™n?",
    "Khi h·ªçc m·ªôt k·ªπ nƒÉng m·ªõi, b·∫°n ∆∞u ti√™n tr·∫£i nghi·ªám tr·ª±c ti·∫øp hay h√¨nh dung c√°c kh·∫£ nƒÉng tr∆∞·ªõc? Gi·∫£i th√≠ch.",
    "H√£y k·ªÉ l·∫°i l·∫ßn b·∫°n gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ b·∫±ng c√°ch d·ª±a v√†o th·ª±c t·∫ø tr∆∞·ªõc m·∫Øt thay v√¨ gi·∫£ thuy·∫øt xa.",
    "ƒê∆∞a ra v√≠ d·ª• khi b·∫°n tin t∆∞·ªüng tr·ª±c gi√°c h∆°n s·ªë li·ªáu; ƒëi·ªÅu g√¨ khi·∫øn b·∫°n quy·∫øt ƒë·ªãnh nh∆∞ v·∫≠y?",
    "Khi ph·∫£i ph√°n ƒëo√°n m·ªôt √Ω t∆∞·ªüng g√¢y tranh c√£i, b·∫°n ƒë√°nh gi√° d·ª±a tr√™n nguy√™n t·∫Øc logic n√†o?",
    "M√¥ t·∫£ m·ªôt t√¨nh hu·ªëng b·∫°n ƒë·∫∑t ∆∞u ti√™n cho c·∫£m x√∫c con ng∆∞·ªùi thay v√¨ l·∫≠p lu·∫≠n l√Ω tr√≠ thu·∫ßn t√∫y.",
    "B·∫°n ƒë·ªãnh nghƒ©a \"c√¥ng b·∫±ng\" nh∆∞ th·∫ø n√†o trong m·ªôt cu·ªôc tranh lu·∫≠n quan tr·ªçng?",
    "H√£y k·ªÉ l·∫ßn b·∫°n ph·∫£i th√¥ng b√°o quy·∫øt ƒë·ªãnh kh√≥ khƒÉn cho ng∆∞·ªùi kh√°c v√† c√°ch b·∫°n c√¢n b·∫±ng gi·ªØa kh√°ch quan v√† ƒë·ªìng c·∫£m.",
    "Bu·ªïi s√°ng ƒëi l√†m, b·∫°n c√≥ l·ªãch tr√¨nh c·ªë ƒë·ªãnh hay th√≠ch linh ho·∫°t t√πy h·ª©ng? Minh h·ªça b·∫±ng v√≠ d·ª•.",
    "Khi nh·∫≠n d·ª± √°n d√†i h·∫°n, b·∫°n l·∫≠p k·∫ø ho·∫°ch chi ti·∫øt ƒë·∫øn m·ª©c n√†o tr∆∞·ªõc khi b·∫Øt ƒë·∫ßu?",
    "K·ªÉ v·ªÅ l·∫ßn b·∫°n thay ƒë·ªïi k·∫ø ho·∫°ch v√†o ph√∫t ch√≥t ‚Äì ƒëi·ªÅu g√¨ th√¥i th√∫c b·∫°n linh ho·∫°t?",
    "B·∫°n c·∫£m th·∫•y th·∫ø n√†o khi kh√¥ng k·ªãp ho√†n th√†nh m·ª•c ti√™u ƒë√∫ng h·∫°n, v√† b·∫°n ph·∫£n ·ª©ng ra sao?",
    "M√¥ t·∫£ \"m·ªôt ng√†y ho√†n h·∫£o\" theo b·∫°n, t·ª´ khi th·ª©c d·∫≠y ƒë·∫øn l√∫c ƒëi ng·ªß; h√£y ch√∫ √Ω m·ª©c ƒë·ªô c·∫•u tr√∫c vs. ng·∫´u h·ª©ng trong ng√†y ƒë√≥."
]

def initialize_session_state():
    """Initialize session state variables"""
    if 'pipeline' not in st.session_state:
        st.session_state.pipeline = None
    if 'pipeline_initialized' not in st.session_state:
        st.session_state.pipeline_initialized = False
    if 'analysis_history' not in st.session_state:
        st.session_state.analysis_history = []
    if 'survey_responses' not in st.session_state:
        st.session_state.survey_responses = {}
    if 'survey_completed' not in st.session_state:
        st.session_state.survey_completed = False
    if 'mbti_result' not in st.session_state:
        st.session_state.mbti_result = None

def initialize_pipeline():
    """
    Initialize the MBTI pipeline
    
    Returns:
        tuple: (success: bool, message: str)
    """
    try:
        # Check if pipeline is already initialized
        if st.session_state.get('pipeline_initialized', False):
            return True, "Pipeline ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o tr∆∞·ªõc ƒë√≥."
            
        # Check if dataset exists - use absolute path
        base_dir = Path(__file__).parent
        dataset_path = base_dir / "mbti_dataset" / "mbti_responses_800.json"
        dataset_path = dataset_path.resolve()  # Convert to absolute path
        
        if not dataset_path.exists():
            error_msg = f"Kh√¥ng t√¨m th·∫•y t·∫≠p d·ªØ li·ªáu MBTI t·∫°i {dataset_path}. Vui l√≤ng ƒë·∫£m b·∫£o file t·ªìn t·∫°i."
            logger.error(error_msg)
            return False, error_msg
        
        logger.info(f"ƒêang kh·ªüi t·∫°o pipeline v·ªõi d·ªØ li·ªáu t·ª´: {dataset_path}")
        
        # Initialize pipeline
        try:
            pipeline = MBTIPipeline(
                data_dir=str(dataset_path.parent),
                semantic_model_name="all-MiniLM-L6-v2",
                device=None,  # Auto-detect device
                enable_caching=True
            )
            
            # Explicitly call initialize to ensure everything is set up
            if not pipeline.initialize():
                error_msg = "Kh√¥ng th·ªÉ kh·ªüi t·∫°o pipeline. Vui l√≤ng ki·ªÉm tra log ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt."
                logger.error(error_msg)
                return False, error_msg
                
            st.session_state.pipeline = pipeline
            st.session_state.pipeline_initialized = True
            
            # Get pipeline stats
            try:
                stats = pipeline.get_pipeline_stats()
                st.session_state.pipeline_stats = stats
                logger.info(f"Pipeline stats: {stats}")
            except Exception as stats_error:
                logger.warning(f"Kh√¥ng th·ªÉ l·∫•y th·ªëng k√™ pipeline: {str(stats_error)}")
            
            logger.info("Kh·ªüi t·∫°o pipeline th√†nh c√¥ng")
            return True, "Kh·ªüi t·∫°o pipeline th√†nh c√¥ng!"
            
        except Exception as init_error:
            error_msg = f"L·ªói khi kh·ªüi t·∫°o pipeline: {str(init_error)}"
            logger.error(error_msg, exc_info=True)
            return False, error_msg
        
    except Exception as e:
        error_msg = f"L·ªói kh√¥ng x√°c ƒë·ªãnh khi kh·ªüi t·∫°o pipeline: {str(e)}"
        logger.error(error_msg, exc_info=True)
        return False, error_msg

def format_responses_for_analysis(responses: Dict[str, str]) -> str:
    """Format survey responses for MBTI analysis"""
    combined_text = ""
    for i, (question, answer) in enumerate(responses.items(), 1):
        combined_text += f"C√¢u {i}: {question}\nTr·∫£ l·ªùi: {answer}\n\n"
    return combined_text.strip()

def analyze_mbti_responses(responses: Dict[str, str]) -> Dict:
    """Analyze MBTI survey responses using the pipeline"""
    if not st.session_state.pipeline_initialized:
        st.error("Pipeline not initialized. Please initialize it first.")
        return None
    
    # Format responses for analysis
    combined_text = format_responses_for_analysis(responses)
    
    try:
        with st.spinner("Analyzing your MBTI personality..."):
            # Use pipeline to analyze the combined responses
            result = st.session_state.pipeline.analyze_text(
                combined_text, 
                k=10,  # Get more examples for better analysis
                semantic_weight=0.7
            )
            
            # Map the result keys to match what the UI expects
            mapped_result = {
                'analysis': {
                    'similar_responses': result.get('top_similar', []),
                    'summary': result.get('analysis', {}).get('summary', 'No analysis available')
                },
                'predicted_type': predict_mbti_type(result)
            }
            
            # Add any additional fields that might be needed
            if 'query_text' in result:
                mapped_result['query_text'] = result['query_text']
            if 'processed_text' in result:
                mapped_result['processed_text'] = result['processed_text']
            
            return mapped_result
            
    except Exception as e:
        st.error(f"Analysis failed: {e}")
        return None

def predict_mbti_type(analysis_result: Dict) -> str:
    """Predict MBTI type based on analysis results"""
    if not analysis_result:
        return "Unknown"
        
    # Check both old and new result structures
    similar_responses = analysis_result.get('similar_responses') or analysis_result.get('top_similar', [])
    if not similar_responses:
        return "Unknown"
        
    # Count MBTI types from similar responses
    type_counter = {}
    for response in similar_responses:
        mbti_type = response.get('mbti_type')
        if mbti_type:
            score = response.get('final_score', 1.0)  # Default score of 1.0 if not provided
            if mbti_type in type_counter:
                type_counter[mbti_type] += score
            else:
                type_counter[mbti_type] = score
    
    # Return the most common type, or 'Unknown' if no types found
    if not type_counter:
        return "Unknown"
        
    return max(type_counter.items(), key=lambda x: x[1])[0]
    
    # Count MBTI types from similar responses
    mbti_counts = {}
    for response in analysis_result['similar_responses']:
        mbti_type = response.get('mbti_type', 'Unknown')
        if mbti_type != 'Unknown':
            mbti_counts[mbti_type] = mbti_counts.get(mbti_type, 0) + response.get('hybrid_score', 0)
    
    if not mbti_counts:
        return "Unknown"
    
    # Return the MBTI type with highest weighted score
    predicted_type = max(mbti_counts, key=mbti_counts.get)
    return predicted_type

def survey_interface():
    """MBTI Survey interface"""
    st.header("üß† MBTI Personality Survey")
    st.markdown("*Tr·∫£ l·ªùi 15 c√¢u h·ªèi sau ƒë·ªÉ kh√°m ph√° t√≠nh c√°ch MBTI c·ªßa b·∫°n*")
    
    if not st.session_state.survey_completed:
        st.info("H√£y tr·∫£ l·ªùi t·∫•t c·∫£ 15 c√¢u h·ªèi m·ªôt c√°ch chi ti·∫øt v√† ch√¢n th·∫≠t ƒë·ªÉ c√≥ k·∫øt qu·∫£ ch√≠nh x√°c nh·∫•t.")
        
        # Survey form
        with st.form("mbti_survey"):
            responses = {}
            
            for i, question in enumerate(MBTI_QUESTIONS, 1):
                st.subheader(f"C√¢u h·ªèi {i}")
                st.write(question)
                
                # Get existing response if available
                existing_response = st.session_state.survey_responses.get(f"q{i}", "")
                
                response = st.text_area(
                    f"C√¢u tr·∫£ l·ªùi {i}:",
                    value=existing_response,
                    height=100,
                    key=f"response_{i}",
                    placeholder="H√£y tr·∫£ l·ªùi chi ti·∫øt v√† c·ª• th·ªÉ..."
                )
                responses[question] = response
            
            submitted = st.form_submit_button("Ph√¢n t√≠ch t√≠nh c√°ch MBTI", type="primary")
            
            if submitted:
                # Validate responses
                empty_responses = [i+1 for i, (_, response) in enumerate(responses.items()) if not response.strip()]
                
                if empty_responses:
                    st.error(f"Vui l√≤ng tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß c√°c c√¢u h·ªèi: {', '.join(map(str, empty_responses))}")
                else:
                    # Save responses
                    st.session_state.survey_responses = {f"q{i+1}": response for i, (_, response) in enumerate(responses.items())}
                    
                    # Analyze responses
                    analysis_result = analyze_mbti_responses(responses)
                    
                    if analysis_result:
                        # Predict MBTI type
                        predicted_mbti = predict_mbti_type(analysis_result)
                        
                        # Save results
                        st.session_state.mbti_result = {
                            'predicted_type': predicted_mbti,
                            'analysis': analysis_result,
                            'timestamp': time.time()
                        }
                        
                        st.session_state.survey_completed = True
                        st.rerun()
    
    else:
        # Show results
        st.success("üéâ Kh·∫£o s√°t ƒë√£ ho√†n th√†nh!")
        
        if st.session_state.mbti_result:
            result = st.session_state.mbti_result
            
            # Display predicted MBTI type
            st.subheader("üéØ K·∫øt qu·∫£ ph√¢n t√≠ch t√≠nh c√°ch")
            
            col1, col2 = st.columns([1, 2])
            
            with col1:
                st.metric(
                    "Lo·∫°i t√≠nh c√°ch MBTI c·ªßa b·∫°n:",
                    result['predicted_type'],
                    help="K·∫øt qu·∫£ d·ª±a tr√™n ph√¢n t√≠ch c√°c ph·∫£n h·ªìi c·ªßa b·∫°n"
                )
            
            with col2:
                # Show confidence based on top matches
                if result['analysis']['similar_responses']:
                    top_scores = [r.get('hybrid_score', 0) for r in result['analysis']['similar_responses'][:3]]
                    avg_confidence = sum(top_scores) / len(top_scores) if top_scores else 0
                    st.metric(
                        "ƒê·ªô tin c·∫≠y:",
                        f"{avg_confidence:.1%}",
                        help="M·ª©c ƒë·ªô ph√π h·ª£p v·ªõi c√°c m·∫´u t∆∞∆°ng t·ª± trong d·ªØ li·ªáu"
                    )
            
            # Show detailed analysis
            st.subheader("üìä Ph√¢n t√≠ch chi ti·∫øt")
            
            if result['analysis']['similar_responses']:
                st.write("**C√°c m·∫´u t∆∞∆°ng t·ª± nh·∫•t v·ªõi ph·∫£n h·ªìi c·ªßa b·∫°n:**")
                
                for i, response in enumerate(result['analysis']['similar_responses'][:5], 1):
                    with st.expander(f"M·∫´u {i}: MBTI {response.get('mbti_type', 'Unknown')} (ƒê·ªô ph√π h·ª£p: {response.get('hybrid_score', 0):.1%})"):
                        st.write("**N·ªôi dung:**", response.get('chunk_text', '')[:300] + "...")
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            if 'semantic_similarity' in response:
                                st.metric("T∆∞∆°ng ƒë·ªìng ng·ªØ nghƒ©a", f"{response['semantic_similarity']:.1%}")
                        with col2:
                            if 'style_similarity' in response:
                                st.metric("T∆∞∆°ng ƒë·ªìng phong c√°ch", f"{response['style_similarity']:.1%}")
            
            # Show generated analysis prompt
            with st.expander("ü§ñ Prompt ph√¢n t√≠ch ƒë∆∞·ª£c t·∫°o"):
                st.code(result['analysis']['prompt'], language="markdown")
            
            # Restart survey option
            st.subheader("üîÑ L√†m l·∫°i kh·∫£o s√°t")
            if st.button("L√†m l·∫°i kh·∫£o s√°t t·ª´ ƒë·∫ßu", type="secondary"):
                # Reset survey state
                st.session_state.survey_responses = {}
                st.session_state.survey_completed = False
                st.session_state.mbti_result = None
                st.rerun()

def analyze_text_interface():
    """Text analysis interface"""
    st.header("üîç Text Analysis")
    
    # Text input
    text_input = st.text_area(
        "Enter text to analyze:",
        placeholder="Type or paste the text you want to analyze for MBTI personality traits...",
        height=150
    )
    
    # Analysis parameters
    col1, col2 = st.columns(2)
    with col1:
        k_similar = st.slider("Number of similar examples", 1, 10, 5)
    with col2:
        semantic_weight = st.slider("Semantic vs Style weight", 0.0, 1.0, 0.7)
    
    if st.button("Analyze Text", type="primary"):
        if not text_input.strip():
            st.warning("Please enter some text to analyze.")
            return
        
        if not st.session_state.pipeline_initialized:
            st.error("Pipeline not initialized. Please initialize it first.")
            return
        
        try:
            with st.spinner("Analyzing text..."):
                result = st.session_state.pipeline.analyze_text(
                    text_input, 
                    k=k_similar, 
                    semantic_weight=semantic_weight
                )
            
            # Display results
            st.success("Analysis completed!")
            
            # Store in history
            st.session_state.analysis_history.append({
                'timestamp': time.time(),
                'text': text_input[:100] + "..." if len(text_input) > 100 else text_input,
                'result': result
            })
            
            # Show similar responses
            if result['similar_responses']:
                st.subheader("üìä Similar Responses Found")
                
                for i, response in enumerate(result['similar_responses'], 1):
                    with st.expander(f"Match {i}: MBTI {response.get('mbti_type', 'Unknown')} (Score: {response.get('hybrid_score', 0):.3f})"):
                        st.write("**Text:**", response.get('chunk_text', '')[:200] + "...")
                        if 'semantic_similarity' in response:
                            st.write("**Semantic Similarity:**", f"{response['semantic_similarity']:.3f}")
                        if 'style_similarity' in response:
                            st.write("**Style Similarity:**", f"{response['style_similarity']:.3f}")
            
            # Show generated prompt
            with st.expander("ü§ñ Generated Analysis Prompt"):
                st.code(result['prompt'], language="markdown")
            
        except Exception as e:
            st.error(f"Analysis failed: {e}")

def comparison_interface():
    """Text comparison interface"""
    st.header("‚öñÔ∏è Text Comparison")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("Text 1")
        text1 = st.text_area(
            "First text:",
            key="text1",
            placeholder="Enter the first text...",
            height=150
        )
    
    with col2:
        st.subheader("Text 2")
        text2 = st.text_area(
            "Second text:",
            key="text2",
            placeholder="Enter the second text...",
            height=150
        )
    
    if st.button("Compare Texts", type="primary"):
        if not text1.strip() or not text2.strip():
            st.warning("Please enter both texts to compare.")
            return
        
        if not st.session_state.pipeline_initialized:
            st.error("Pipeline not initialized. Please initialize it first.")
            return
        
        try:
            with st.spinner("Comparing texts..."):
                comparison = st.session_state.pipeline.compare_texts(text1, text2)
            
            st.success("Comparison completed!")
            
            # Show comparison prompt
            with st.expander("ü§ñ Generated Comparison Prompt"):
                st.code(comparison['comparison_prompt'], language="markdown")
            
            # Show individual analyses
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("Text 1 Analysis")
                if comparison['analysis1']['similar_responses']:
                    top_match = comparison['analysis1']['similar_responses'][0]
                    st.info(f"Top match: {top_match.get('mbti_type', 'Unknown')} (Score: {top_match.get('hybrid_score', 0):.3f})")
            
            with col2:
                st.subheader("Text 2 Analysis")
                if comparison['analysis2']['similar_responses']:
                    top_match = comparison['analysis2']['similar_responses'][0]
                    st.info(f"Top match: {top_match.get('mbti_type', 'Unknown')} (Score: {top_match.get('hybrid_score', 0):.3f})")
            
        except Exception as e:
            st.error(f"Comparison failed: {e}")

def history_interface():
    """Analysis history interface"""
    st.header("üìù Analysis History")
    
    # Include survey results in history
    all_history = []
    
    # Add survey result if exists
    if st.session_state.mbti_result:
        all_history.append({
            'type': 'survey',
            'timestamp': st.session_state.mbti_result['timestamp'],
            'result': st.session_state.mbti_result
        })
    
    # Add text analysis history
    for item in st.session_state.analysis_history:
        all_history.append({
            'type': 'text_analysis',
            'timestamp': item['timestamp'],
            'text': item['text'],
            'result': item['result']
        })
    
    if not all_history:
        st.info("No analysis history yet. Complete the MBTI survey or analyze some texts to see them here!")
        return
    
    # Clear history button
    if st.button("Clear History", type="secondary"):
        st.session_state.analysis_history = []
        st.session_state.mbti_result = None
        st.session_state.survey_completed = False
        st.session_state.survey_responses = {}
        st.rerun()
    
    # Sort by timestamp (newest first)
    all_history.sort(key=lambda x: x['timestamp'], reverse=True)
    
    # Display history
    for i, item in enumerate(all_history, 1):
        timestamp = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(item['timestamp']))
        
        if item['type'] == 'survey':
            with st.expander(f"MBTI Survey {i} - {timestamp}"):
                st.write("**Predicted MBTI Type:**", item['result']['predicted_type'])
                if item['result']['analysis']['similar_responses']:
                    st.write("**Top matches:**")
                    for j, response in enumerate(item['result']['analysis']['similar_responses'][:3], 1):
                        st.write(f"{j}. MBTI {response.get('mbti_type', 'Unknown')} (Score: {response.get('hybrid_score', 0):.3f})")
        else:
            with st.expander(f"Text Analysis {i} - {timestamp}"):
                st.write("**Text:**", item['text'])
                
                if item['result']['similar_responses']:
                    st.write("**Top matches:**")
                    for j, response in enumerate(item['result']['similar_responses'][:3], 1):
                        st.write(f"{j}. MBTI {response.get('mbti_type', 'Unknown')} (Score: {response.get('hybrid_score', 0):.3f})")

def settings_interface():
    """Settings and configuration interface"""
    st.header("‚öôÔ∏è Settings")
    
    # Pipeline status
    st.subheader("Pipeline Status")
    if st.session_state.pipeline_initialized:
        stats = st.session_state.pipeline.get_pipeline_stats()
        
        col1, col2 = st.columns(2)
        with col1:
            st.success("‚úÖ Pipeline Initialized")
            st.json(stats)
        
        with col2:
            if st.button("Reinitialize Pipeline"):
                st.session_state.pipeline_initialized = False
                st.session_state.pipeline = None
                st.rerun()
    else:
        st.warning("‚ùå Pipeline Not Initialized")
        if st.button("Initialize Pipeline"):
            if initialize_pipeline():
                st.rerun()
    
    # Gemini API status
    st.subheader("Gemini API Configuration")
    try:
        key_manager = APIKeyManager()
        st.success(f"‚úÖ {key_manager.get_key_count()} API keys available")
    except Exception as e:
        st.error(f"‚ùå Gemini API configuration error: {e}")
        st.info("Please configure your .env file with valid Gemini API keys")
    
    # Survey data management
    st.subheader("Survey Data")
    if st.session_state.survey_responses:
        st.write(f"**Responses saved:** {len(st.session_state.survey_responses)} answers")
        if st.button("Clear Survey Data"):
            st.session_state.survey_responses = {}
            st.session_state.survey_completed = False
            st.session_state.mbti_result = None
            st.success("Survey data cleared!")
            st.rerun()
    else:
        st.info("No survey data available")
    
    # About
    st.subheader("About")
    st.info("""
    This MBTI Personality Analysis app uses a sophisticated pipeline combining:
    - **15-Question Survey** for comprehensive personality assessment
    - **Semantic embeddings** for content understanding
    - **Style embeddings** for writing pattern analysis  
    - **Hybrid retrieval** for finding similar personality examples
    - **Prompt engineering** for structured analysis
    
    The system processes responses through Unicode normalization, chunking, 
    embedding creation, similarity search, and deduplication to provide 
    accurate personality insights based on the Myers-Briggs Type Indicator.
    """)

def main():
    """Main app function"""
    # Initialize session state
    initialize_session_state()
    
    # App header
    st.title("üß† MBTI Personality Analyzer")
    st.markdown("*Kh√°m ph√° t√≠nh c√°ch c·ªßa b·∫°n th√¥ng qua kh·∫£o s√°t 15 c√¢u h·ªèi*")
    
    # Initialize pipeline if not already done
    if not st.session_state.pipeline_initialized:
        with st.spinner("ƒêang kh·ªüi t·∫°o h·ªá th·ªëng ph√¢n t√≠ch..."):
            success, message = initialize_pipeline()
            if not success:
                st.error(f"L·ªói: {message}")
                st.info("Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n ƒë·∫øn t·∫≠p d·ªØ li·ªáu v√† th·ª≠ l·∫°i.")
                return
        
        # Show pipeline stats if initialization was successful
        if 'pipeline_stats' in st.session_state:
            stats = st.session_state.pipeline_stats
            st.success("‚úÖ H·ªá th·ªëng ƒë√£ s·∫µn s√†ng!")
            
            # Show pipeline stats in columns
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Tr·∫°ng th√°i", "ƒê√£ s·∫µn s√†ng")
            with col2:
                st.metric("C∆° s·ªü d·ªØ li·ªáu ng·ªØ nghƒ©a", f"{stats['semantic_db']['total_documents']} m·∫´u")
            with col3:
                st.metric("C∆° s·ªü d·ªØ li·ªáu phong c√°ch", f"{stats['style_db']['total_documents']} m·∫´u")
    
    # Show the survey interface
    survey_interface()
    
    # Footer
    st.sidebar.markdown("---")
    st.sidebar.markdown("**MBTI Personality Survey**")
    st.sidebar.markdown("Built with Streamlit & Python")

if __name__ == "__main__":
    # Allow running the Streamlit app via `python app.py`
    import os
    import sys
    import subprocess
    if os.getenv("RUNNING_STREAMLIT_APP") != "true":
        os.environ["RUNNING_STREAMLIT_APP"] = "true"
        subprocess.run(["streamlit", "run", sys.argv[0]])
    else:
        main()
